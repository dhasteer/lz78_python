{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LZ78 Sequential Probability Assignment: Python Implementation\n",
    "This code is associated with the paper [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589).\n",
    "\n",
    "This codebase is in Python, which is more popular than Rust. This Python codebase gives users the option to experiment more comfortably with implementation modifications at the cost of slower runtime (on the order of about 3-5x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "1. (Optional) Set up and activate a virtual environment for this project.\n",
    "2. Install the `lz78_python` package: `pip install --editable .`. Note that the `--editable` option allows you to implementation modifications to propagate to the package without having to rerun `pip install .`.\n",
    "\n",
    "You should be all set! This tutorial will walk you through the functionalities that the Python codebase offers parallel to the Rust codebase functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lorem requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lz78_python.utils.CharacterMap import CharacterMap\n",
    "from lz78_python.naive.encoder import LZ78_encode\n",
    "from lz78_python.naive.decoder import LZ78_decode\n",
    "from lz78_python.streamed.encoder import BlockLZ78Encoder\n",
    "from lz78_python.spa.encoder import LZ78SPA\n",
    "import lorem, bitarray, requests\n",
    "from os import makedirs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sequences\n",
    "\n",
    "This class does not explicitly exist in this version of the codebase. We can directly use Python lists for integer sequences and Python strings for character sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Example: Integer Sequence\n",
    "\n",
    "We will not go into depth with this example, given that you should be able to recreate the same behaviors through Python list.\n",
    "\n",
    "However, Python list does not have a direct method to check the number of unique symbols (ie. alphabet size), but you can always do `len(set(lst))` for that functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 `CharacterMap`\n",
    "\n",
    "Underlying logic assumes an integer representation of a sequence, so we need a way to map strings to integer-based sequences ranging from `0` to `A-1`.\n",
    "\n",
    "The `CharacterMap` class maps characters in a string to integer values in a contiguous range, so that a string can be used as an individual sequence.\n",
    "It has the capability to **encode** a string into the corresponding integer representation, and **decode** a list of integers into a string.\n",
    "\n",
    "Inputs:\n",
    "- data: a string consisting of all of the characters that will appear in the character map. For instance, a common use case is:\n",
    "    ```\n",
    "    charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some dummy data and make a character map\n",
    "s = \" \".join(([lorem.paragraph() for _ in range(10)]))\n",
    "charmap = CharacterMap(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `encode`\n",
    "Takes a string and returns the corresponding integer representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.encode(\"lorem ipsum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It errors if any characters to be encoded are not in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.encode(\"hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `filter_string`\n",
    "Takes a string and removes any characters that are not present in the character mapping.\n",
    "This is useful if you have some text with special characters, and you don't want the special characters to be in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.filter_string(\"hello world. Lorem ipsum! @#$%^&*()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `decode`\n",
    "Decodes an integer representation of a string into the string itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.decode(charmap.encode(\"lorem ipsum\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `alphabet_size`\n",
    "Returns how many characters can be represented by the character mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.alphabet_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Example: Character Sequence\n",
    "\n",
    "A string-based sequence in Python is represented simply as a string. We will need to define a `CharacterMap` for this sequence and pass it into all sequence-related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ?,\")\n",
    "charseq = \" \".join(([lorem.paragraph() for _ in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the integer representation of the corresponding characters, we use the `CharacterMap.encode` function, but indexing a character sequence directly in this implementation will return the characters of the character sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap.encode(charseq[100:130])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charseq[100:130], charmap.decode(charmap.encode(charseq[100:130]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have a `Sequence` object, there is no `CharacterMap` explicitly tied in with the Python string (that serves as the character sequence in this implementation). The burden will be on the user to keep track of the `CharacterMap`'s corresponding to the strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LZ78 Compression\n",
    "\n",
    "The `LZ78Encoder` object performs plain LZ78 encoding and decoding, as described in \"Compression of individual sequences via variable-rate coding\" (Ziv, Lempel 1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 `CompressedSequence` tuple object\n",
    "\n",
    "The `CompressedSequence` object stores an encoded bitstream and compression ratio of the encoding compared the original uncompressed data. `CompressedSequence` objects cannot be instantiated directly, but rather are returned by `LZ78Encoder.encode`.\n",
    "\n",
    "The main functionality is:\n",
    "1. Getting the compression ratio as `(encoded size) / (uncompressed len * log A)`,\n",
    "    where A is the size of the alphabet.\n",
    "2. Getting a `bitarray.bitarray` representing this object, so that the compressed\n",
    "    sequence can be stored to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \" \".join(([lorem.paragraph() for _ in range(10_000)]))\n",
    "charmap = CharacterMap(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `LZ78Encoder` Instance method: `encode`\n",
    "Performs LZ78 encoding on an individual sequence, and returns a `LZ78Output` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = LZ78_encode(data, custom_char_map=charmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Compressed Sequence` Tuple attribute: `compression_ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.compression_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving a `CompressedSequence` tuple object\n",
    "\n",
    "This is a parallel to the `to_bytes` and `from_bytes` functionality offered by the Rust implementation. We use the underlying `bitarray.bitarray` representation to directly write the LZ78 encoded bits to a file and read from it. \n",
    "\n",
    "However, unlike the Rust `CompressedSequence` implementation, reading the data from the file does not return the original `CompressedSequence` tuple object; it only returns the encoded `bitarray.bitarray`. (So, we would be losing out on immediate compression ratio information.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: reading bitarray from file may include additional 0's \n",
    "# because of padding when writing the data to file, so we \n",
    "# should also track length and apply it when loading the bits\n",
    "encoded_bitlength = len(encoded.bits)\n",
    "makedirs(\"test_data\", exist_ok=True)\n",
    "with open(\"test_data/saved_encoded_sequence.bin\", 'wb') as file:\n",
    "    encoded.bits.tofile(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the compressed sequence from the file and decode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = bitarray.bitarray()\n",
    "with open(\"test_data/saved_encoded_sequence.bin\", 'rb') as file:\n",
    "    bits.fromfile(file)\n",
    "bits = bits[:encoded_bitlength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = LZ78_decode(\n",
    "    bits,\n",
    "    alphabet_size=charmap.alphabet_size(), \n",
    "    return_str=True,\n",
    "    custom_char_map=charmap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert decoded == data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Block-Wise Compression\n",
    "Sometimes, it might be useful to loop through blocks of data and perform LZ78 encoding on each block (e.g., if you need to do data processing before LZ78 compression and want to have some sort of pipeline parallelism).\n",
    "\n",
    "The `BlockLZ78Encoder` has this functionality: you can pass in the input sequence to be compressed in chunks, and the output (`encoder.get_encoded_sequence()`) is as if the full concatenated sequence was passed in to an LZ78 encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ,?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = BlockLZ78Encoder(\n",
    "    alphabet_size=charmap.alphabet_size(),\n",
    "    input_is_string=True,\n",
    "    custom_char_map=charmap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `encode_block`\n",
    "Encodes a block using LZ78, starting at the end of the previous block.\n",
    "\n",
    "All blocks must be over the same alphabet, or else the call to `encode_block` will error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    encoder.encode_block(lorem.paragraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.encode_block([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_encoded_sequence`\n",
    "Returns the compressed sequence, which is equivalent to the output of `LZ78Encoder.encode` on the concatenation of all inputs to `encode_block` thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.get_encoded_sequence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance method: `compression_ratio`\n",
    "\n",
    "The encoder contains information about the `compression_ratio`, not the encoded bit `bitarray.bitarray`, unlike the `CompressedSequence` object of the Rust implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.compression_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method: `LZ78_decode`\n",
    "Decompresses the compressed sequence that has been constructed thus far. This is the same method used for decoding the \"naive\" (all-in-one-go) encoding. This is not an instance method, unlike the `decode` function of the Rust implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = LZ78_decode(\n",
    "    encoder.get_encoded_sequence(),\n",
    "    alphabet_size=charmap.alphabet_size(), \n",
    "    return_str=True,\n",
    "    custom_char_map=charmap,\n",
    ")\n",
    "print(decoded[376:400])\n",
    "charmap.encode(decoded[376:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LZ78 Sequential Probability Assignment (SPA)\n",
    "The `LZ78SPA` class is the implementation of the family of sequential probability assignments discussed in [A Family of LZ78-based Universal Sequential Probability Assignments](https://arxiv.org/abs/2410.06589), for Dirichlet priors.\n",
    "In this section, `gamma` refers to the Dirichlet parameter.\n",
    "\n",
    "Under this prior, the sequential probability assignment is an additive\n",
    "perturbation of the emprical distribution, conditioned on the LZ78 prefix\n",
    "of each symbol (i.e., the probability model is proportional to the\n",
    "number of times each node of the LZ78 tree has been visited, plus gamma).\n",
    "\n",
    "This SPA has the following capabilities:\n",
    "- training on one or more sequences,\n",
    "- log loss (\"perplexity\") computation for test sequences,\n",
    "- SPA computation (using the LZ78 context reached at the end of parsing\n",
    "    the last training block),\n",
    "- sequence generation.\n",
    "\n",
    "Note that the LZ78SPA does not perform compression; you would have to use\n",
    "a separate BlockLZ78Encoder object to perform block-wise compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Example: LZ78 SPA on Markov Data\n",
    "\n",
    "We will use the Markov probability source used in [(Rajaraman et al, 2024)](https://arxiv.org/pdf/2404.08335), where the transition probability depends solely on $x_{t-k}$.\n",
    "Specifically, $x_t = x_{t-k}$ with probability $0.9$, and otherwise $x_t$ is picked uniformly at random from the rest of the alphabet.\n",
    "\n",
    "The SPA works best when the alphabet size is $2$, but you can try out other alphabet sizes too.\n",
    "\n",
    "First, we define some helper functions for generating the data (don't worry about understanding these; they are irrelevant to understanding the SPA itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods for generating data; feel free run the cell without\n",
    "# reading the code\n",
    "def sample_index_from_dist(probabilities):\n",
    "    cdf = np.cumsum(probabilities)\n",
    "    cdf[-1] = 1 # in case of FP error\n",
    "    return int(np.where(np.random.random() < cdf)[0][0])\n",
    "\n",
    "def entropy(probs):\n",
    "    return sum([-x * np.log2(x) for x in probs if x > 0])\n",
    "\n",
    "def get_stationary_dist(transition_probabilities):\n",
    "    eigvals, eigvecs = np.linalg.eig(transition_probabilities.T)\n",
    "    # all eigenvalues will be <= 1, and one will be =1\n",
    "    stationary_dist = eigvecs[:, np.argmax(eigvals)]\n",
    "    return stationary_dist / sum(stationary_dist)\n",
    "\n",
    "def entropy_rate(transition_probabilities):\n",
    "    stationary_dist = get_stationary_dist(transition_probabilities)\n",
    "    return sum([prob * entropy(transition_probabilities[i]) \n",
    "                for i, prob in enumerate(stationary_dist)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data to pass through the SPA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can change these\n",
    "ALPHABET_SIZE = 2\n",
    "PEAK_PROB = 0.9\n",
    "K = 5\n",
    "N = 1_000_000\n",
    "N_TEST = 10_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build data array; feel free to ignore this code and just run the cell\n",
    "transition_probabilities = np.eye(ALPHABET_SIZE) * PEAK_PROB + \\\n",
    "    (np.ones((ALPHABET_SIZE, ALPHABET_SIZE)) - np.eye(ALPHABET_SIZE)) * (1 - PEAK_PROB) / (ALPHABET_SIZE - 1)\n",
    "start_prob = np.ones(ALPHABET_SIZE) / ALPHABET_SIZE\n",
    "\n",
    "data = np.zeros(N, dtype=int)\n",
    "for i in range(K):\n",
    "    data[i] = sample_index_from_dist(start_prob)\n",
    "for i in range(K,N):\n",
    "    data[i] = sample_index_from_dist(transition_probabilities[data[i-K]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class: `LZ78SPA`\n",
    "\n",
    "Observe the possible inputs to the LZ78SPA class. It is slightly different than that of the Rust implementation. For instance, you must provide a `CharacterMap` to the SPA in this case (if dealing with character sequences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(LZ78SPA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `train_on_block`\n",
    "\n",
    "Use a block of data to update the SPA. If `include_prev_context` is\n",
    "true, then this block is considered to be from the same sequence as\n",
    "the previous. Otherwise, it is assumed to be a separate sequence, and\n",
    "we return to the root of the LZ78 prefix tree at the start of training.\n",
    "\n",
    "It returns the self-entropy log loss incurred while processing this\n",
    "sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = LZ78SPA()\n",
    "spa.train_on_block(data[:-N_TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `compute_test_loss`\n",
    "After training a SPA, you can compute the log loss of a test sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa.compute_test_loss(\n",
    "    data[-N_TEST:], \n",
    "    len(data[-N_TEST:]),\n",
    "    include_prev_context=True\n",
    ") / N_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_normalized_log_loss`\n",
    "Gets the normalized self-entropy log loss incurred from training the SPA thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa.get_normalized_log_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `get_prob_for_next_symbol`\n",
    "\n",
    "Computes the SPA for the specified symbol at the alphabet, using the LZ78 context reached at the end of parsing the last training block. To achieve the functionality of the `compute_spa_at_current_state` Rust method, we can apply a list comprehension over calculating `get_prob_for_next_symbol` over the symbols in the alphabet\n",
    "\n",
    "In this case, the list comprehension will return a two-element list, where the first element is the estimated probability that the next symbol is $0$ and the second is the estimated probability that the next symbol is $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[spa.get_prob_for_next_symbol(i) for i in range(ALPHABET_SIZE)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the Rust implementation, the Python implementation does not offer a `to_bytes` instance method for the `LZ78SPA` class and by extension a `spa_from_bytes` method to recover the SPA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Example: Text Generation\n",
    "\n",
    "Let's use the LZ78 SPA to generate some text based on Sherlock Holmes novels.\n",
    "\n",
    "This requires the `requests` library and an internet connection.\n",
    "If you don't have either, you can perform the same experiment any text you'd like, including the lorem ipsum text from the beginning of this tutorial.\n",
    "Just make sure you have enough training data (e.g., the Sherlock novel used for this example is 500 kB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(\"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our own character map and filter the text based on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "charmap = CharacterMap(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ. ,?\\n\\\"';:\\t-_\")\n",
    "filtered_text = charmap.filter_string(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the SPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spa = LZ78SPA(\n",
    "    alphabet_size=charmap.alphabet_size(), \n",
    "    gamma=0.2, \n",
    "    custom_char_map=charmap,\n",
    "    input_is_string=True,\n",
    ")\n",
    "spa.train_on_block(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instance method: `generate_data`\n",
    "Generates a sequence of data, using temperature and top-k sampling (see\n",
    "the \"Experiments\" section of [Sagan and Weissman 2024] for more details).\n",
    "\n",
    "Inputs:\n",
    "- **len**: number of symbols to generate\n",
    "- **min_context** (optional): the SPA tries to maintain a context of at least a\n",
    "    certain length at all times. So, when we reach a leaf of the LZ78\n",
    "    prefix tree, we try traversing the tree with different suffixes of\n",
    "    the generated sequence until we get a sufficiently long context\n",
    "    for the next symbol.\n",
    "- **temperature** (optional): a measure of how \"random\" the generated sequence is. A\n",
    "    temperature of 0 deterministically generates the most likely\n",
    "    symbols, and a temperature of 1 samples directly from the SPA.\n",
    "    Temperature values around 0.1 or 0.2 function well.\n",
    "- **top_k** (optional): forces the generated symbols to be of the top_k most likely\n",
    "    symbols at each timestep.\n",
    "- **seed_data** (optional): you can specify that the sequence of generated data\n",
    "be the continuation of the specified sequence.\n",
    "\n",
    "Returns a tuple of the generated sequence and that sequence's log loss,\n",
    "or perplexity.\n",
    "\n",
    "Errors if the SPA has not been trained so far, or if the seed data is\n",
    "not over the same alphabet as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(generated, loss) = spa.generate_data(\n",
    "    500,\n",
    "    min_context=5,\n",
    "    temperature=0.1,\n",
    "    top_k=5,\n",
    "    seed_data=\"This \"\n",
    ")\n",
    "generated = \"This \" + generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now, the generated text will not appear very high-quality for a few reasons:\n",
    "1. We don't have a lot of training data. LZ78-based algorithms generally take a lot of data to train; empirically, the SPA does better with many megabytes of data, at least.\n",
    "2. The `min_context` parameter is being used very heuristically, and could potentially be forcing us to the bottom of the LZ78 tree, where we don't have a lot of data, making the SPA closer to uniform. If you see long gibberish strings, try decreasing `min_context`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(generated), 80):\n",
    "    print(generated[i:i+80])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
